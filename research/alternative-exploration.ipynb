{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    '/Users/uditrawat/Desktop/CardioX/artifacts/data_ingestion/heart_2020_cleaned.csv')\n",
    "\n",
    "df.drop(['AgeCategory', 'Race', 'GenHealth'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocess the data (split, encode, etc.)\n",
    "X = df.drop(columns=['HeartDisease'],axis=1)\n",
    "y = df['HeartDisease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    '/Users/uditrawat/Desktop/CardioX/artifacts/data_transformation/test.csv')\n",
    "X_test = data.drop(columns=['HeartDisease'])\n",
    "y_test = data['HeartDisease']\n",
    "data_s = pd.read_csv(\n",
    "    '/Users/uditrawat/Desktop/CardioX/artifacts/data_transformation/train.csv')\n",
    "X_train = data_s.drop(columns=['HeartDisease'])\n",
    "y_train = data_s['HeartDisease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 selected features: ['Stroke', 'PhysicalHealth', 'MentalHealth', 'DiffWalking', 'Diabetic', 'KidneyDisease']\n"
     ]
    }
   ],
   "source": [
    "# Convert target to binary labels\n",
    "y = y.apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply Label Encoding (if needed)\n",
    "le = LabelEncoder()\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype == 'object':\n",
    "        X_train[col] = le.fit_transform(X_train[col])\n",
    "        X_test[col] = le.transform(X_test[col])\n",
    "\n",
    "# Step 2: Feature Selection using Chi-Square\n",
    "chi_selector = SelectKBest(chi2, k=6)\n",
    "X_train_top6 = chi_selector.fit_transform(X_train, y_train)\n",
    "X_test_top6 = chi_selector.transform(X_test)\n",
    "\n",
    "top6_features = X_train.columns[chi_selector.get_support()]\n",
    "print(f\"Top 5 selected features: {list(top6_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'subsample': 0.8, 'scale_pos_weight': 10.683076080007307, 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0.1, 'colsample_bytree': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'scale_pos_weight': [scale_pos_weight_xgb * 0.8, scale_pos_weight_xgb, scale_pos_weight_xgb * 1.2],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],   # Step size shrinkage\n",
    "    'n_estimators': [100, 200, 300],       # Number of trees\n",
    "    'max_depth': [3, 5, 7],                # Maximum tree depth\n",
    "    # Minimum sum of instance weight needed in a child\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    # Minimum loss reduction required for a split\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    # Fraction of samples used for training each tree\n",
    "    'subsample': [0.8, 1.0],\n",
    "    # Fraction of features used for each tree\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Perform Randomized Search with cross-validation\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    xgb_model, param_grid_xgb, cv=5, n_iter=50, scoring='roc_auc', random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the RandomizedSearchCV on training data\n",
    "random_search_xgb.fit(X_train_top6, y_train)\n",
    "\n",
    "# Best parameters from random search\n",
    "print(f\"Best parameters for XGBoost: {random_search_xgb.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'subsample': 1.0,\n",
       "  'scale_pos_weight': 12.819691296008768,\n",
       "  'reg_lambda': 0.1,\n",
       "  'reg_alpha': 0.1,\n",
       "  'num_leaves': 31,\n",
       "  'n_estimators': 200,\n",
       "  'min_child_weight': 3,\n",
       "  'max_depth': 3,\n",
       "  'learning_rate': 0.05,\n",
       "  'colsample_bytree': 0.8},\n",
       " '\\n',\n",
       " {'subsample': 0.8,\n",
       "  'scale_pos_weight': 10.683076080007307,\n",
       "  'n_estimators': 100,\n",
       "  'min_child_weight': 5,\n",
       "  'max_depth': 3,\n",
       "  'learning_rate': 0.1,\n",
       "  'gamma': 0.1,\n",
       "  'colsample_bytree': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (XGBoost):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.76      0.85     58484\n",
      "           1       0.20      0.63      0.30      5475\n",
      "\n",
      "    accuracy                           0.75     63959\n",
      "   macro avg       0.58      0.69      0.57     63959\n",
      "weighted avg       0.89      0.75      0.80     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use only XGBoost model for prediction\n",
    "xgb_model_ex = XGBClassifier(**random_search_xgb, random_state=42)\n",
    "\n",
    "# Fit the XGBoost model\n",
    "xgb_model_ex.fit(X_train_top6, y_train)\n",
    "\n",
    "# Make predictions using XGBoost\n",
    "y_pred_xgb_proba = xgb_model_ex.predict_proba(X_test_top6)[:, 1]\n",
    "\n",
    "# Convert probabilities to binary predictions based on a threshold (0.5)\n",
    "y_pred_xgb = (y_pred_xgb_proba >= 0.5).astype(int)\n",
    "\n",
    "# Step 5: Evaluate the XGBoost model using classification report\n",
    "print(\"Classification Report (XGBoost):\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_xgb = {'subsample': 0.8,\n",
    "  'scale_pos_weight': 10.683076080007307,\n",
    "  'n_estimators': 100,\n",
    "  'min_child_weight': 5,\n",
    "  'max_depth': 3,\n",
    "  'learning_rate': 0.1,\n",
    "  'gamma': 0.1,\n",
    "  'colsample_bytree': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (XGBoost):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.76      0.85     58484\n",
      "           1       0.20      0.63      0.30      5475\n",
      "\n",
      "    accuracy                           0.75     63959\n",
      "   macro avg       0.58      0.69      0.57     63959\n",
      "weighted avg       0.89      0.75      0.80     63959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use only XGBoost model for prediction\n",
    "xgb_model_ex = XGBClassifier(**random_search_xgb, random_state=42)\n",
    "\n",
    "# Fit the XGBoost model\n",
    "xgb_model_ex.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using XGBoost\n",
    "y_pred_xgb_proba = xgb_model_ex.predict_proba(X_test_top6)[:, 1]\n",
    "\n",
    "# Convert probabilities to binary predictions based on a threshold (0.5)\n",
    "y_pred_xgb = (y_pred_xgb_proba >= 0.5).astype(int)\n",
    "\n",
    "# Step 5: Evaluate the XGBoost model using classification report\n",
    "print(\"Classification Report (XGBoost):\")\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conclusion of the day\n",
    "#transformation pipeline is faulty\n",
    "#task to do - correct the pipeline, push train and transform pipeline, work on evaluation pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cardiox-e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
